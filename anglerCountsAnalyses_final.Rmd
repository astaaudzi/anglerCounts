---
title: "Comparing different methods of angler counting"
author: "Asta, Fernando, Justas and others"
output:
  html_document:
    code_folding: hide
    code_download: true
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: no
  md_document:
    variant: markdown_github
  pdf_document:
    toc: yes



knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "output") })

---

```{r global_options, echo=FALSE}
knitr::opts_chunk$set(fig.width=6, fig.asp = 0.618, fig.path='output/images/',
                      echo=TRUE, warning=FALSE, message=FALSE, cache= FALSE)
  

options(allow_html_in_all_outputs=TRUE)
```

# Libraries

Fernando - which packages do we actually need?

```{r library}

#rm(list = ls()) # clear memory
#install.packages("pacman")
#writeLines(pacman::p_lib(), "~/Desktop/list_of_R_packages.csv") # to quickly back up packages

#pacman::p_load(tidyverse,ggplot2, tidyr, janitor, data.table, here, skimr, lubridate, sf, leaflet, mapview, styler, readxl, ggrepel, units, lwgeom, cowplot, osmdata, maps) # just add needed packages to this line and Pacman will install and load them.

# pacman::p_load(tidyverse,ggplot2, tidyr, janitor, data.table, here, skimr, lubridate, sf, leaflet, mapview, styler, readxl, ggrepel, units, lwgeom, geos, htmltools, viridis, RColorBrewer, gganimate, tmap, ggmap, rgdal, "cowplot", "googleway", "ggplot2", "ggrepel", "ggspatial", "libwgeom", "sf", "rnaturalearth", "rnaturalearthdata", rgeos, osmdata, maps, rnaturalearthhires)

pacman::p_load(tidyverse,ggplot2, tidyr, janitor, data.table, here, skimr, lubridate, sf, leaflet, mapview, styler, readxl, ggrepel, units, lwgeom, geos, htmltools, viridis, RColorBrewer, gganimate, tmap, ggmap, rgdal, "cowplot", "googleway", "ggplot2", "ggrepel", "ggspatial", "sf", "rnaturalearth", "rnaturalearthdata", rgeos, osmdata, maps,raster, ggsn, ggthemes, effects, MuMIn, gdata, grid, ggplotify, rasterVis, compareDF, diffdf)


```

# Fig S1: map

Map of Central Europe with area around Kaunas Reservoir as an inset. 
the main map shows Kaunas Water reservoir as light gray as well as  
- drone trajectories
- spots for visual counts on ice

Fernando can you please modify the final figure 1 with a few points I indicated below? Thanks

```{r fig1, echo=TRUE}

drone_north <- read_xlsx("data/PlannedTrajectory_north.xlsx")
drone_south <- read_excel("data/PlannedTrajectory_south.xlsx")
visual_coords <- read.table(file = "data/visual_coordinates.txt") %>% 
  rename(Latitude = V1, Longitude = V2) %>% 
  mutate(Latitude = str_sub(Latitude,1, nchar(Latitude)-1)) %>% 
  mutate(across(where(is.character), as.numeric))

#Alternative terrain background

droneTrajectories <- read.table(file = "data/droneTrajectory/KaunoMarios_MisijaWhole.txt")
drones.sf <- st_as_sf(droneTrajectories, coords = c("V2", "V1"), crs = 4326)
height <- max(droneTrajectories$V1) - min(droneTrajectories$V1)
width <- max(droneTrajectories$V2) - min(droneTrajectories$V2)
kau_borders <- c(bottom  = min(droneTrajectories$V1)  - 0.5 * height,
                 top     = max(droneTrajectories$V1)  + 0.5 * height,
                 left    = min(droneTrajectories$V2) - 1.3 * width,
                 right   = max(droneTrajectories$V2) + 0.5 * width)

#remove first and last trajectory points for better line plotting
drone_south <- drone_south[c(3:97),]
drone_north <- drone_north[c(3:67),]
#remove visual observation spots outside the surveyed area
visual_coords1 <- visual_coords %>% filter (Longitude > 24.10)


stamen <- get_stamenmap(kau_borders, zoom = 11, maptype = "terrain-background")

## Fernando, can we add a scale to this main map? We need to add a line indicating 5km scale. Also rename axes labels to longitude and latitude

g1 <- ggmap(stamen)+
    geom_path(drone_south, mapping = aes (x = Longitude, y = Latitude), color = "yellow", size = 1.0 )+
  geom_path(drone_north, mapping = aes (x = Longitude, y = Latitude), color = "blue", size = 1.0)+
  geom_point(visual_coords1, mapping = aes (x = Longitude, y = Latitude), color = "red", size = 2.5 ) +
  geom_text(x=23.97, y=54.91, label= "Kaunas city", size = 5, fontface = 2) +
    theme(
    panel.grid.major = element_blank(), 
    panel.grid.minor = element_blank(),
    axis.title.x = element_text(size=14),
    axis.title.y = element_text(size=14),
    axis.text=element_text(size=8)
  )

g1  

# Creating inset

world <- ne_countries(scale = "medium", returnclass = "sf")
europe_cropp <- st_crop(world, xmin = 5, xmax = 35,
                                  ymin = 50, ymax = 63)

kau_shape <- here("data" , "kaunas", "Kaunas_water_reservoir.shp") %>%
  st_read() %>% 
  st_transform(4326) #transform crs to wgs84
 
lit_shape <- here("data" , "lithuania", "lt_100km.shp") %>%
  st_read() %>%
  st_transform(4326) #transform crs to wgs84

kau_bb = st_as_sfc(st_bbox(kau_shape)) 
lit_bb = st_as_sfc(st_bbox(lit_shape)) 

g2 <-   ggplot() +
  geom_sf(data = europe_cropp, fill = "gray96", color = "gray60", size = 0.3) +
  geom_sf(data = lit_bb, fill = NA, color = "blue", size = 0.8) +
  geom_sf(data = kau_bb, fill = "darkblue", color = "darkblue", size = 1) +
  theme_void()

g2

## Fernando, some work is needed here too please. We need some fill for the inset and move it to the bottom left corner. Also 
## combine plots
fig1 <- ggdraw() +
  draw_plot(g1) +
  draw_plot(g2, x = 0.15, y = 0.1, width = 0.4, height = 0.4)

fig1
#ggsave(here ("output", "images", "fig1.png"), dpi = "print")


# data_map <- gdata::combine(drone_north,drone_south,visual_coords) %>% 
#   mutate(source = as.factor(source))

# Kaunas main map  
#kaunas_map <- fortify(kau_shape) #'fortify' the data to get a dataframe format required by ggplot2

# cols <- c("red", "blue", "green")

# g2 <- ggplot2::ggplot() +
#   geom_sf(data = kaunas_map, fill = "light blue", color = "black", size = 0.5) +
#   # geom_point(data_map, mapping = aes (x = Longitude, y = Latitude, color = source)) +
#   geom_path(drone_south, mapping = aes (x = Longitude, y = Latitude), color = "yellow2", size = 1.5 )+
#   geom_path(drone_north, mapping = aes (x = Longitude, y = Latitude), color = "yellow2", size = 1.5 )+
#   geom_point(visual_coords1, mapping = aes (x = Longitude, y = Latitude), color = "blue", size = 2 ) +
#   theme_light() + theme(panel.grid.major = element_blank(),
#                         panel.grid.minor = element_blank(), 
#                         axis.title.x = element_blank(),
#                         axis.title.y = element_blank(),
#     axis.text=element_text(size=10))


```


# Read and process drone data and coordinates

This chunk does not need to be run, because processed data is saved into drones_df_full.RData files

```{r drone_deeper, include = F, eval = F, echo = F}
# first read in all drone data, including note on whether the angler is on the shore or in the boat 

# load drone data
# Sys.setlocale(, "lithuanian")

# rvest::guess_encoding(here("data", "drone_data","Pavasaris", "20200505.xlsx"))

file.list <- list.files(path = here("data", "drone_data"), pattern='*.xlsx', recursive = TRUE, full.names = TRUE)

file.list <- setNames(file.list, file.list) # only needed when you need an id-column with the file-names

drones_raw <- map_df(file.list, read_xlsx, .id = "id") # had to change Ziema to read

#2983 a total number of anglers

drones_df <- drones_raw %>% 
  janitor::clean_names() %>% 
  dplyr::select(1:9) %>% 
  mutate(date_name = lubridate::ymd(data)) %>% 
  dplyr::rename(lat = latitude) %>% 
dplyr::rename(lon = longitude) %>% 
  dplyr::mutate(type = "drone") %>% 
  drop_na(lat) %>% #remove empty rows
tidyr::uncount(weights = kiekis) %>% # single row for each fisherman in kiekis (number of fishermen observed in each point)
  dplyr::select(3:10)

# fix the number of anglers in boats. After uncount command the number of boat anglers was not uncounted
drones_df$boat <- 0
drones_df$boat[which(drones_df$ju_tarpe_valtyje > 0)] <- 1

#only select relevant columns for analyses
drones_df <- drones_df %>% dplyr::select(1:4, 7:9)

#compare with drone summary file provided separately

drone_summary <- drones_df %>% group_by(date_name) %>% summarise(anglers = n())
#on 2020-07-24 the total number of anglers in the summary file was 54, i get 56. This is because I corrected two cases of "kiekis" set to 0, even when there were coordinates of shore anlers recorded. Presumably the summary file also just summed all "kiekis' counts. 

## save 
# save(drones_df, file = "data/drones_df_full.RData")
# 
# write.csv(drones_df, here("data", "drones_df_full.csv"))
```

### Get deeper coordinates for drone data

This section does not need to be run, and full echosounder data cannot be provided publicly due to data sharing agreement with the company. The section just shows how the coordinates are extracted for the specific days when drone surveys were conducted. It saves files at the end

```{r, eval = F, echo = F, include = F}

multipoint_df <- readRDS( file = "otherData/multipoint_df.rds")

#filter deeper and drone days 

 deeper_df<- multipoint_df %>% 
 dplyr::select(trip_id, user_id, lat, lon, date_name, hour_of_day, duration) %>% 
  dplyr::semi_join(drones_df, by= "date_name") %>% 
  dplyr::mutate(type = "deeper") %>% 
   dplyr::distinct(user_id, date_name,lat, lon, .keep_all = T)
 

 # filter by hour of day
 
 deeper_df_612 <- deeper_df %>% 
       dplyr::filter(hour_of_day >= 6, hour_of_day <= 12)   #  filter by hour range
 
  # Bind deeper and dron dfs
 
 deeper_drone_df <- dplyr::bind_rows(deeper_df,drones_df) %>% 
  mutate(lat = round(lat,4)) %>% 
  mutate(lon = round(lon,4)) %>% 
  mutate(across(c(user_id, date_name, trip_id,  type),factor)) %>% 
  distinct() %>% 
   drop_na(lat|lon)

 # Hour filtered version
 
 deeper_drone_df_612 <- dplyr::bind_rows(deeper_df_612,drones_df) %>% 
  mutate(lat = round(lat,4)) %>% 
  mutate(lon = round(lon,4)) %>% 
  mutate(across(c(user_id, date_name, trip_id,  type),factor)) %>% 
  distinct() %>% 
   drop_na(lat|lon)
 
 
 # Create spatial objects

 deeper_drone_sf <- st_as_sf(deeper_drone_df, coords = c("lon", "lat"),  crs=4326)
 deeper_drone_sf_612 <- st_as_sf(deeper_drone_df_612, coords = c("lon", "lat"),  crs=4326)
 
 # Load shapefile of kaunas
 
 kaunas_shp<- here("data" , "arcgis_layers", "Kaunas_water_reservoir.shp") %>%
  st_read() %>% 
  st_transform(4326) #transform crs to wgs84
 
# Load shapefile of drone surveyed area
 
 drone_shp<- here("data" , "arcgis_layers", "drone_flights.shp") %>%
  st_read() %>% 
  st_transform(4326) #transform crs to wgs84
 
# filter points within Kaunas
 
deeper_drone_kaunas <- 
  st_join(deeper_drone_sf, kaunas_shp, join = st_within) %>% 
  filter(!is.na(NAM)) # spatial join to get intersection of points and poly
 
 
# filter points within surveyed area
 
deeper_drone_surveyed_area <- 
  st_join(deeper_drone_sf, drone_shp, join = st_within) %>% 
  filter(!is.na(NAM)) # spatial join to get intersection of points and poly

# filter points within area and time range

 
deeper_drone_surveyed_area_612 <- 
  st_join(deeper_drone_sf_612, drone_shp, join = st_within) %>% 
  filter(!is.na(NAM)) # spatial join to get intersection of points and poly

# save(deeper_drone_kaunas, file = "data/deeper_drone_kaunas40days.RData")
# save(deeper_drone_surveyed_area, file = "data/deeper_drone_kaunas_survarea.RData")
# save(deeper_drone_surveyed_area_812, file = "data/deeper_drone_kaunas_surv612.RData")

```

## Fig. S2: Map drone vs deeper

Fernando, I cannot find these RData files in the directory anglerCounts_git/data or anywehre in our shared folders
I found three csv files - are they the same? I can see them on github online, but they are 3 months old, so I am not sure these are the latest files. Can you please check that this chunk can be run and data is available. We also need to produce a few plots saved to the output/ folder which I can add to the manuscript

```{r map_drone}

load(file = "data/drones_df_full.RData")
load(file = "data/deeper_drone_kaunas40days.RData")
load(file = "data/deeper_drone_kaunas_survarea.RData")
load(file = "data/deeper_drone_kaunas_surv612.RData")

 deeper_drone_sf <- st_as_sf(deeper_drone_kaunas, coords = c("lon", "lat"),  crs=4326)
 deeper_drone_sf_612 <- st_as_sf(deeper_drone_surveyed_area_612, coords = c("lon", "lat"),  crs=4326)

  kaunas_shp<- here("data" , "arcgis_layers", "Kaunas_water_reservoir.shp") %>%
  st_read() %>% 
  st_transform(4326) #transform crs to wgs84
 

pal <- colorFactor(
 palette = c('red', 'blue'),
  domain = deeper_drone_sf$type
)


#select one of these days 
unique(deeper_drone_kaunas$date_name)
#  [1] 2020-11-30 2020-04-25 2021-01-23 2021-01-24 2021-01-30 2021-01-21 2020-05-09 2020-08-22 2020-09-02 2020-04-24 2020-05-30 2020-07-24
# [13] 2020-12-27 2020-06-27 2020-05-26 2020-10-24 2020-08-14 2020-11-05 2020-03-28 2020-10-03 2020-06-16 2020-05-05 2020-05-21 2020-06-07
# [25] 2020-07-18 2020-07-09 2020-11-21 2020-03-23 2020-12-13 2020-09-19 2020-05-15 2020-08-04 2020-08-28 2020-09-24 2020-12-09 2020-10-14
# [37] 2020-09-11 2021-02-19 2021-02-24 2021-02-27

#deeper_drone_surveyed_area_812 %>% 
#deeper_drone_surveyed_area %>% 
deeper_drone_kaunas %>% 
   filter(date_name %in% c("2021-01-21")) %>% 
leaflet() %>% 
  addTiles() %>%
  addCircles(weight = 2, 
             radius = 3, color = ~pal(type)) %>% 
 addScaleBar(position = "topright") #%>%
  #addLegend("bottomright", pal = pal, values = ~type,
   # title = "Echosounder vs. drone observations",
 # title = "Echosounder vs. drone observations, between 8-12h",
    #opacity = 1)


```

# Table S2: Drone vs.land-based stats

```{r dronevsland}

visdr <- read.csv(file = "data/visual_drone.csv")
colnames(visdr) <- c("visual", "drone", "mission", "method")

unique(visdr$method)

#first compare the total count
totalc <- visdr %>% filter(method == "total")
t.test(totalc$visual, totalc$drone , paired = F, alternative = "two.sided")

## here are alternatives and options 
?t.test
# If var.equal is TRUE then the pooled estimate of the variance is used. By default, if var.equal is FALSE then the variance is estimated separately for both groups and the Welch modification to the degrees of freedom is used.
#http://www.sthda.com/english/wiki/unpaired-two-samples-t-test-in-r 
#more details on the Welch modification of the t-test

t.test(totalc$visual, totalc$drone , paired = F, var.equal = F, alternative = "two.sided")


#compare boat count
boatc <- visdr %>% filter(method == "no_boats") %>% filter (mission != "mis4" & mission != "mis5")
t.test(boatc$visual, boatc$drone , paired = F, var.equal = T, alternative = "two.sided")


#compare boat anglers
boata <- visdr %>% filter(method == "boat_anglers") %>% filter (mission != "mis4" & mission != "mis5")
t.test(boata$visual, boata$drone , paired = F, var.equal = F,  alternative = "two.sided")

#compare shore anglers
shora <- visdr %>% filter(method == "shore_anglers") %>% filter (mission != "mis4" & mission != "mis5")
t.test(shora$visual, shora$drone , paired = F, var.equal = F, alternative = "two.sided")

#compare ice anglers
icea <- visdr %>% filter(method == "ice_anglers") %>% filter (mission == "mis4" | mission == "mis5")
t.test(icea$visual, icea$drone , paired = F, var.equal = T, alternative = "two.sided")

```

### Angler model from drones

some information on linear regression 
https://www.datacamp.com/community/tutorials/linear-regression-R 
http://www.learnbymarketing.com/tutorials/linear-regression-in-r/
a short summary of distributions and links
https://www.statmethods.net/advstats/glm.html
And this one is very good!
https://cran.r-project.org/web/packages/jtools/vignettes/summ.html


```{r, eval = T, echo = T}
library(lme4)
library(effects)
library(MuMIn)
library(MASS)
library(jtools)
library(sandwich)

drone <- read.csv(file = "data/DroneSummary.csv", fileEncoding="UTF-8-BOM")
drone$date2 <- as.Date(drone$Date, "%d/%m/%Y") 

## get basic drone statistics
mean(drone$Anglers)
median(drone$Anglers)
sd(drone$Anglers)
max(drone$Anglers)
min(drone$Anglers)
sum(drone$Anglers)

#let's fit a simple model to estimate the number of anglers depending on weekday, season, weather

drone$season <- as.factor(drone$season)
drone$Ice <- as.factor(drone$Ice)
drone$weekday <- as.factor(drone$weekday)
drone$weather <- as.factor(drone$weather)
is.numeric(drone$Temp)

#First we need to decide which distribution we want to assume for the anglers. If we use basic linear model we assume normal distribution, which means that the numbers can go to minus to plus infinity (there can be a negative number of anglers observed). Let's see how they are distributed. Most likely we need to assume lognomral distribution. This means that the logarith of the value should be approximately normally distributed. Lognormal distribution cannot take negative values
hist(drone$Anglers, breaks = 20)
#for count data people recommend using Poisson distribution, but it does not work for us this distribution assumes that mean is about equal to variance. This is certainly not true in our case
mean(drone$Anglers)
var(drone$Anglers)

hist(log(drone$Anglers), breaks = 30)

# Full model 
anglerModel1 <- lm(log(Anglers) ~ 1+ season + Ice*weekday + weather*weekday + Temp, data = drone)
summary(anglerModel1) #this model explains 35% of variance
plot(anglerModel1)  
#we can see that observations 32 and especially 37, 38 are clear outliers. So we might consider removing them 

drone2 <- drone[-c(37,38),]
anglerModel1 <- lm(log(Anglers) ~ 1+ season + Ice*weekday + weather*weekday + Temp, data = drone2)
summary(anglerModel1) #this one explains 60% of variance after removing two outliers and median of residuals is exactly 0 which is perfect (we want residuals to be centered around 0)
plot(anglerModel1) #looks pretty good 

options(na.action=na.fail) # need to run this before dredge
#now let's "dredge" the model, which means removing various variables and selecting the best 
dredge(anglerModel1)

# ok, so we can see that we have two best models, one with Ice, weekends and ice-weekend interaction and the second one (same AICc value) with only ice. So let's look at both of them. 

#only model with weekend
ModelWeekend <- lm(log(Anglers) ~ 1 + weekday, data = drone2)
summary(ModelWeekend)
#or we use this cool command from library jtools
summ(ModelWeekend)
#this one explains 22% of variance 
plot(ModelWeekend)

#only model with ice
ModelIce <- lm(log(Anglers) ~ 1 + Ice, data = drone2)
summ(ModelIce)  #not useful at all!
summary(ModelIce)

#model with weekends and ice interaction
ModelBest <- lm(log(Anglers) ~ 1 + Ice*weekday, data = drone2)
#and the same model but without excluding outliers
ModelBestFull <- lm(log(Anglers) ~ 1 + Ice*weekday, data = drone)

summ(ModelBest)
summ(ModelBestFull)
plot(ModelBestFull)


summary(ModelBest)
summary(ModelBestFull)
anova(ModelBest, ModelWeekend) #this is only marginally better than only weekends, but p is not a best evaluator here
plot(ModelBest) #looks well distributed
plot(ModelBestFull) #looks well distributed

PredictAngler<- as.data.frame(Effect(c('weekday', 'Ice'),ModelBest))
#save(PredictAngler, file = "output/Predict_ModelBest.RData")

PredictAnglerFull<- as.data.frame(Effect(c('weekday', 'Ice'),ModelBestFull))
#save(PredictAnglerFull, file = "output/Predict_ModelBestFull.RData")

PredictSecondBest<- as.data.frame(Effect(c('weekday'),ModelWeekend))
#save(PredictSecondBest, file = "output/Predict_ModelSecondBest.RData")


```

## Fig 2: model prediction

Note the x axis position of the observations will vary every time you produce a plot because we use geom_jitter command to spread the points randomly along the x axis. 

```{r, eval = T, echo = T}
load(file = "output/Predict_ModelBest.RData")
load(file = "output/Predict_ModelSecondBest.RData")

drone <- read.csv(file = "data/DroneSummary.csv")
drone$date2 <- as.Date(drone$Ã¯..Date, "%d/%m/%Y") 
drone$season <- as.factor(drone$season)
drone$Ice <- as.factor(drone$Ice)
drone$weekday <- as.factor(drone$weekday)
drone$weather <- as.factor(drone$weather)
drone2 <- drone[-c(37,38),]

drone <- drone %>%
  # Rename Ice levels
  mutate(Ice = recode(Ice, "Ice" = "Ice", "Open_water" = "Open water"))


PredictAngler <- PredictAngler %>%
  # Rename Ice levels
  mutate(Ice = recode(Ice, "Ice" = "Ice", "Open_water" = "Open water"))


PredictAngler <- PredictAnglerFull %>%
  # Rename Ice levels
  mutate(Ice = recode(Ice, "Ice" = "Ice", "Open_water" = "Open water"))



violin_plot <- ggplot() + 
  geom_violin(data = drone, aes (x = weekday, y = Anglers), width = 0.8, 
              fill='#d3d3d3', color="darkblue", trim = FALSE) +
  # geom_boxplot(data=drone,aes (x = weekday, y = Anglers), width=0.1, color="blue", alpha=0.9) +
  # geom_dotplot(data=drone,aes (x = weekday, y = Anglers), binaxis='y', stackdir='center', dotsize=1) +
  geom_jitter(data=drone,aes (x = weekday, y = Anglers),shape=16, position=position_jitter(0.2), alpha=0.5, col="blue") +
  geom_point(data = PredictAngler, aes (x = weekday, y = exp(fit)), col = 'red', size = 3, alpha = 0.5) +
  geom_errorbar(data = PredictAngler, aes(x = weekday, ymin = exp(lower), ymax = exp(upper)), col = 'red', size = 1, alpha = 0.5, width = 0.2) +
  facet_wrap(~Ice) + 
  labs(x = "", y = "Number of anglers") + 
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"), legend.position = "none") +
    scale_x_discrete(limits = c("0", "1"),
                   labels = c("Weekday", "Weekend"))

violin_plot
ggsave(here ("output", "images", "fig2_violin_fullData.png"), dpi = "print")

## and the same but only for the model with weekdays

violin_plot_second <- ggplot() + 
  geom_violin(data = drone, aes (x = weekday, y = Anglers), width = 0.8, 
              fill='#d3d3d3', color="darkblue", trim = FALSE) +
  # geom_boxplot(data=drone,aes (x = weekday, y = Anglers), width=0.1, color="blue", alpha=0.9) +
  # geom_dotplot(data=drone,aes (x = weekday, y = Anglers), binaxis='y', stackdir='center', dotsize=1) +
  geom_jitter(data=drone,aes (x = weekday, y = Anglers),shape=16, position=position_jitter(0.2), alpha=0.5, col="blue") +
  geom_point(data = PredictSecondBest, aes (x = weekday, y = exp(fit)), col = 'red', size = 3, alpha = 0.5) +
  geom_errorbar(data = PredictSecondBest, aes(x = weekday, ymin = exp(lower), ymax = exp(upper)), col = 'red', size = 1, alpha = 0.5, width = 0.2) +
  labs(x = "", y = "Number of anglers") + 
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"), legend.position = "none") +
    scale_x_discrete(limits = c("0", "1"),
                   labels = c("Weekday", "Weekend"))

violin_plot_second


ggsave(here ("output", "images", "fig2_violin_second.png"), dpi = "print")

```

### Table 1: predict angler numbers

```{r}
## Model with weekend and ice interaction, two outlier days excluded
load(file = "output/Predict_ModelBest.RData")
## Model with weekend effect only, two outlier days excluded
load(file = "output/Predict_ModelSecondBest.RData")
## Model with weekend and ice interaction, no outlier points excluded
load(file = "output/Predict_ModelBestFull.RData")

# Daily records of sonar users in three datasets (same area and mornings only - deeper3, same area all day - deeper2, and all water reservoir all day - deeper1)
load(file = "data/OneYearExtrapol_deeper3.RData")

#vector of day weekdays with ice, weekends with ice, weekdays open water, weekend open water
#Let's assume that 10% of days are with ice (about one month of ice cover per year)
#assumed_days <- c(26, 10, 236, 93)

#how many weekdays and weekends of open water and ice fishing?
length(extrapData$weekday[which(extrapData$ice == 0 & extrapData$weekday == 0)])
length(extrapData$weekday[which(extrapData$ice == 0 & extrapData$weekday == 1)])
length(extrapData$weekday[which(extrapData$ice == 1 & extrapData$weekday == 0)])
length(extrapData$weekday[which(extrapData$ice == 1 & extrapData$weekday == 1)])

assumed_days <- c(35, 14, 225, 91) #ice weekdays, ice weekends, open weekdays, open weekends

## or apply only for open water and ice fishing seasons separately
# #to assess for open water season only, set 0 to ice days
# assumed_days <- c(0, 0, 225, 91)
# # for for ice only
# assumed_days <- c(35, 14, 0, 0)

#or we can assume that there is no ice at all 
#assumed_days <- c(0, 0, 262, 103)

PredictAngler$days <- assumed_days

PredictAngler$total_mean <- PredictAngler$days * exp(PredictAngler$fit)
PredictAngler$total_lower <- PredictAngler$days * exp(PredictAngler$lower)
PredictAngler$total_upper <- PredictAngler$days * exp(PredictAngler$upper)

# numbers in the Total number column of Table 1 from linear model
sum(PredictAngler$total_mean)
sum(PredictAngler$total_lower)
sum(PredictAngler$total_upper)

#same with the full model without outliers
PredictAnglerFull$days <- assumed_days

PredictAnglerFull$total_mean <- PredictAnglerFull$days * exp(PredictAnglerFull$fit)
PredictAnglerFull$total_lower <- PredictAnglerFull$days * exp(PredictAnglerFull$lower)
PredictAnglerFull$total_upper <- PredictAnglerFull$days * exp(PredictAnglerFull$upper)

## if we include two outlier days the mean is slightly lower, but confidence intervals are much wider
sum(PredictAnglerFull$total_mean)
sum(PredictAnglerFull$total_lower)
sum(PredictAnglerFull$total_upper)

## Now if we only use the second best model that does not include ice effect we only need two values for extrapolation
#same with the second best model without outliers
assumed_days <- c(262, 103)
PredictSecondBest$days <- assumed_days

PredictSecondBest$total_mean <- PredictSecondBest$days * exp(PredictSecondBest$fit)
PredictSecondBest$total_lower <- PredictSecondBest$days * exp(PredictSecondBest$lower)
PredictSecondBest$total_upper <- PredictSecondBest$days * exp(PredictSecondBest$upper)

## Results are  similar 
sum(PredictSecondBest$total_mean)
sum(PredictSecondBest$total_lower)
sum(PredictSecondBest$total_upper)


#no days with ice (10% of days)
assumed_days <- c(0, 0, 262, 103)
PredictAngler$year <- extr
PredictAngler$lower[which(PredictAngler$lower < 0)] <- 0

PredictAngler$total_mean <- PredictAngler$year * PredictAngler$fit
PredictAngler$total_lower <- PredictAngler$year * PredictAngler$lower
PredictAngler$total_upper <- PredictAngler$year * PredictAngler$upper

sum(PredictAngler$total_mean)
sum(PredictAngler$total_lower)
sum(PredictAngler$total_upper)
# [1] 26082.99
# > sum(PredictAngler$total_lower)
# [1] 19429.52
# > sum(PredictAngler$total_upper)
# [1] 32736.47

```

# Drone vs. echosounder 

Fernando  can you check that this is all working and produces correct datasets? 

This chunk requires full sonar dataset and cannot be run. But it shows steps needed for producing data for analyses. 

Extract 3 level Kaunas user numbers

Starting from 2019-01-01, number of deeper users for
1) all Kaunas reservoir, all day
2) smaller area of Kaunas reservoir (drone surveys), all day
3) smaller area, only 6am to 12pm. 


```{r eval = F, include = F, 3df}

 #load full database and filter

multi3_df <- readRDS( here("data", "total_lithuania_df")) %>% 
  dplyr::filter(year_name %in% c("2020", "2019", "2021")) 

multi3_sf <- st_as_sf(multi3_df, coords = c("lat", "lon"),  crs=4326)


# load kaunas shape

kau_shape <- here("data" , "arcgis_layers", "Kaunas_water_reservoir.shp") %>%
  st_read() %>% 
  st_transform(4326) #transform crs to wgs84


## Filtering data within Kaunas reservoir

kaunas_sf <- 
  st_join(multi3_sf, kau_shape, join = st_within) %>%  # spatial join to get intersection of points and poly
  filter(!is.na(NAM))
  # select(user_id, trip_id, geometry)

# Creating df full kaunas

kaunas3_df <- kaunas_sf %>% 
  st_drop_geometry() %>% # distinct() doesnt like spatial objects
#  dplyr::filter(date_name >= "2021-01-10",date_name <= "2021-02-28") %>%
 dplyr::filter(year_name %in% c("2020", "2019", "2021")) %>% 
  dplyr::distinct(user_id, date_name, .keep_all = T) %>% # to count each user only once per day
 # filter(hour_of_day >= 8, hour_of_day <= 11) %>%  #  filter by hour range
  add_count(date_name, sort = TRUE) %>%  # count users by day
  distinct(date_name, .keep_all = T)  %>%
  rename(deeper_count = "n") %>% 
   dplyr::select(date_name, deeper_count)

write.csv(kaunas3_df, here("data", "kaunas_df.csv"))


kaunas3_df <- kaunas_sf %>% 
  st_drop_geometry() %>% # distinct() doesnt like spatial objects
  dplyr::filter(date_name >= "2021-01-10",date_name <= "2021-02-28") %>%
  dplyr::distinct(user_id, date_name, .keep_all = T)

hist(kaunas3_df$hour_of_day, breaks = 23)

# Load shapefile of drone surveyed area
 
 drone_shp<- here("data" , "arcgis_layers", "drone_flights.shp") %>%
  st_read() %>% 
  st_transform(4326) #transform crs to wgs84
 
 
## Filtering data within drone surveyed area

kaunas_drone_sf <- 
  st_join(multi3_sf, drone_shp, join = st_within) %>%  # spatial join to get intersection of points and poly
  filter(!is.na(NAM))
  # select(user_id, trip_id, geometry)

# Creating df just drone surveys

kaunas3_drone_df <- kaunas_drone_sf %>% 
  st_drop_geometry() %>% # distinct() doesnt like spatial objects
 dplyr::filter(year_name %in% c("2020", "2019", "2021")) %>% 
  dplyr::distinct(user_id, date_name, .keep_all = T) %>% # to count each user only once per day
 # filter(hour_of_day >= 8, hour_of_day <= 11) %>%  #  filter by hour range
  add_count(date_name, sort = TRUE) %>%  # count users by day
  distinct(date_name, .keep_all = T)  %>%
  rename(deeper_count = "n") %>% 
   dplyr::select(date_name, deeper_count)

write.csv(kaunas3_drone_df, here("data", "kaunas_drone_df.csv"))

# Creating df just drone surveys 6-12

kaunas3612_drone_df <- kaunas_drone_sf %>% 
  st_drop_geometry() %>% # distinct() doesnt like spatial objects
 dplyr::filter(year_name %in% c("2020", "2019", "2021")) %>% 
  dplyr::distinct(user_id, date_name, .keep_all = T) %>% # to count each user only once per day
 dplyr::filter(hour_of_day >= 6, hour_of_day <= 12) %>%  #  filter by hour range
  add_count(date_name, sort = TRUE) %>%  # count users by day
  distinct(date_name, .keep_all = T)  %>%
  rename(deeper_count = "n") %>% 
   dplyr::select(date_name, deeper_count)

write.csv(kaunas3612_drone_df, here("data", "kaunas612_drone_df.csv"))

```

### prepare dataset for Bayesian analyses

```{r, eval = F, include = F}

#read in drone data by day
drone <- read.csv(file = "data/DroneSummary.csv", fileEncoding="UTF-8-BOM")
drone$date2 <- as.Date(drone$Date, "%d/%m/%Y") 
drone <- drone %>% dplyr::select (date2, weekday, Anglers, Temp, wind, Ice, season)

#get deeper data for all area all day 
deeper1 <- read.csv(file = "data/kaunas_df.csv", fileEncoding="UTF-8-BOM")
deeper1$date2 <- as.Date(deeper1$date_name, "%Y-%m-%d") 
#add to drone data
drone$deeper1 <- deeper1$deeper_count[match(drone$date2, deeper1$date2)]

## now deeper data for only drone area for all day
deeper2 <- read.csv(file = "data/kaunas_drone_df.csv", fileEncoding="UTF-8-BOM")
deeper2$date2 <- as.Date(deeper2$date_name, "%Y-%m-%d") 
drone$deeper2 <- deeper2$deeper_count[match(drone$date2, deeper2$date2)]

#deeper data for drone area from 6 to 12
deeper3 <- read.csv(file = "data/kaunas612_drone_df.csv", fileEncoding="UTF-8-BOM")
deeper3$date2 <- as.Date(deeper3$date_name, "%Y-%m-%d")
drone$deeper3 <- deeper3$deeper_count[match(drone$date2, deeper3$date2)]

##replace NA deeper days with 0
drone$deeper3[which(is.na(drone$deeper3 == T))] <- 0
drone$deeper2[which(is.na(drone$deeper2 == T))] <- 0
drone$deeper1[which(is.na(drone$deeper1 == T))] <- 0

drone$ice2 <- 0
drone$ice2[which(drone$Ice == "Ice")] <- 1

droneDeeperMain <- drone

#save(droneDeeperMain, file = "data/droneDeeperMain.RData")
Tables1 <- droneDeeperMain[,c(11, 2:8,12:14)] %>% arrange(date2)
knitr::kable(Tables1, digits=2)

```

### Now divide sonar dataset into three categories

For every day of the year starting from 2020-03-01 we will count the number of sonar users in the entire area, surveyed area, and surveyed area mornings only. However, since sonar dataset has some days with no observations we first need to create a dataframe with all days and then merge it with sonar dataset

```{r}
#read in all sonar 3 observations for 2019-2021

extr3 <-  read.csv(here("data", "kaunas612_drone_df.csv"), header = T, sep = ",", dec = ".")
extr2 <-  read.csv(here("data", "kaunas_drone_df.csv"), header = T, sep = ",", dec = ".")
extr1 <-  read.csv(here("data", "kaunas_df.csv"), header = T, sep = ",", dec = ".")

extr1$date_name <- as.Date(extr1$date_name)
extr2$date_name <- as.Date(extr2$date_name)
extr3$date_name <- as.Date(extr3$date_name)

#on some days there are no sonar observations, and these days will not be present in the datafile. So we will create a dataframe with all days and merge with sonar days
days2020 <- seq(as.Date("2020-03-01"), as.Date("2021-02-28"), by="days")
weekd <- weekdays(days2020, abbr = TRUE)
#which(weekd == "Sat" | weekd == "Sun")
W = rep(0, length = length(days2020))
W[which(weekd == "Sat" | weekd == "Sun")] <- 1
test <- as.data.frame(cbind(as.Date(days2020), W))
test$date <- as.Date(days2020)
test <- test[,-1]
colnames(test) <- c("weekday", "date_name")

#add deeper observations
test$deeper1 <- extr1$deeper_count[match(test$date_name, extr1$date_name)]
test$deeper2 <- extr2$deeper_count[match(test$date_name, extr2$date_name)]
test$deeper3 <- extr3$deeper_count[match(test$date_name, extr3$date_name)]

test$deeper3[which(is.na(test$deeper3 == T))] <- 0
test$deeper2[which(is.na(test$deeper2 == T))] <- 0
test$deeper1[which(is.na(test$deeper1 == T))] <- 0

test$ice <- 0
test$ice[which(test$date_name > "2021-01-10")] <- 1

extrapData <- test

# save(data/extrapData, file = "OneYearExtrapol_deeper3.RData")
# write.csv(extrapData, file = "TableSx_DailySonarUseData.csv")

```

### explore three deeper datasets

```{r}
load(file = "data/droneDeeperMain.RData")

droneDeeperMain$ratioAllDrone <- droneDeeperMain$deeper2/droneDeeperMain$deeper1
plot(droneDeeperMain$ratioAllDrone)
which(droneDeeperMain$deeper1 < droneDeeperMain$deeper2)

mean(droneDeeperMain$ratioAllDrone[which(droneDeeperMain$ice2 == 1)])
median(droneDeeperMain$ratioAllDrone[which(droneDeeperMain$ice2 == 1)])
droneDeeperMain$ratioAllDrone[which(droneDeeperMain$ice2 == 1)]

mean(droneDeeperMain$ratioAllDrone[which(droneDeeperMain$ice2 == 0)], na.rm = T)
median(droneDeeperMain$ratioAllDrone[which(droneDeeperMain$ice2 == 0)], na.rm = T)
droneDeeperMain$ratioAllDrone[which(droneDeeperMain$ice2 == 0)]

droneDeeperMain$ratioMornEven <- droneDeeperMain$deeper3/droneDeeperMain$deeper2
plot(droneDeeperMain$ratioMornEven)

mean(droneDeeperMain$ratioMornEven[which(droneDeeperMain$ice2 == 1)], na.rm =T)
median(droneDeeperMain$ratioMornEven[which(droneDeeperMain$ice2 == 1)], na.rm =T)
droneDeeperMain$ratioMornEven[which(droneDeeperMain$ice2 == 1)]

mean(droneDeeperMain$ratioMornEven[which(droneDeeperMain$ice2 == 0)], na.rm =T)
median(droneDeeperMain$ratioMornEven[which(droneDeeperMain$ice2 == 0)], na.rm =T)
droneDeeperMain$ratioMornEven[which(droneDeeperMain$ice2 == 0)]

droneDeeperMain$ratioTotal <- droneDeeperMain$deeper3/droneDeeperMain$deeper1
plot(droneDeeperMain$ratioTotal)

mean(droneDeeperMain$ratioTotal[which(droneDeeperMain$ice2 == 1)], na.rm =T)
median(droneDeeperMain$ratioTotal[which(droneDeeperMain$ice2 == 1)], na.rm =T)
droneDeeperMain$ratioTotal[which(droneDeeperMain$ice2 == 1)]

mean(droneDeeperMain$ratioTotal[which(droneDeeperMain$ice2 == 0)], na.rm =T)
median(droneDeeperMain$ratioTotal[which(droneDeeperMain$ice2 == 0)], na.rm =T)
droneDeeperMain$ratioTotal[which(droneDeeperMain$ice2 == 0)]


```

# ### Bayesian 

# load functions

```{r functions}

#LIKELIHOOD/PRIOR FUNCTIONS ______________________________

log_pdf_exp <- function(x, l){
  # convert input into vector
  x = c(x)
  loglik = log(l)-l*x
  # sum up loglik values to get lik of whole data
  loglik_data = sum(loglik)
  return(loglik_data)
}

log_pdf_uniform <- function(x,lower,upper){ # assumes that x is a single number 
  # convert input into vector
  x = c(x)
  # if the minimum and maximum values of the array lay within the boundaries
  if (min(x)>lower & max(x)<upper){
    loglik = -log(upper-lower)
    # add together one lik value per data point, i.e. multiply lik with length(x)
    loglik_data = loglik*length(x)
  }else{ # if at least one of the values is outside boundaries, the lik is -inf
    loglik_data = -Inf
  }
  return(loglik_data)
}

log_pdf_normal <- function(x,mu,sd){ 
  # convert input into vector
  x = c(x)
  log_lik = -0.5*log(2*pi*sd^2) - ((x - mu)^2)/(2*sd^2)
  # sum up loglik values to get lik of whole data
  loglik_data = sum(log_lik)
  return(loglik_data)
}


log_pdf_gamma <- function(x,a,b){
  # convert input into vector
  x = c(x)
  log_lik = (a-1)*log(x)+(-b*x)-(log(b)*(-a)+ lgamma(a))
  # sum up loglik values to get lik of whole data
  loglik_data = sum(log_lik)
  return(loglik_data)
}


#____________________________PROPOSAL FUNCTIONS________________________________
sliding_window <- function(x,windowsize){
  lower = x-0.5*windowsize
  upper = x+0.5*windowsize
  new_x = runif(1, lower, upper)
  new_x = abs(new_x)
  hastings_ratio = 0
  return(c(new_x,hastings_ratio))
}


multiplier_proposal <- function(i,d=1.2){ # d must be > 1!!
  u <- runif(1)
  l <- 2*log(d)
  m <- exp(l*(u-.5))
  new_x <- i * m
  hastings_ratio <- log(m)
  return( c(new_x, hastings_ratio) )
}

```

### mock data example

since many people struggle to get their heads around log-exp relationship we will create a mock up data and estimate probabilities, to make sure we understand the output 

```{r, eval = F}

load(file = "data/droneDeeperMain.RData")

## drone count
N <- droneDeeperMain$Anglers# drone count
W <- droneDeeperMain$weekday # weekend

# we make up some parameters about the ratio of sonar users and weekend multiplier and plug it into the formula

#our equation is this:
#r_current = r0_current * exp( a_current*W + b_current*D)
S <- round((0.1*N - 0.7*W*0.1*N),0) #and assume that on weekend there are more anglers
round(S/N,2) #now we know the probability exactly

```

# priors and initialise

for this we analyse open water and ice data separately
Or we use this part to compare deeper1 and deeper3

```{r}

# load(file = "data/droneDeeperMain.RData")

## by selecting ice2 = 0 or 1 we chose whether to use open water or ice season 
 droneDeeperMain <- droneDeeperMain[which(droneDeeperMain$ice2 ==0),]
# 
 N <- droneDeeperMain$Anglers # drone count
 S <- droneDeeperMain$deeper3 # sonar count in the surveyed area mornings only
 W <- droneDeeperMain$weekday # weekend

#or if we look at every day deeper usage for the whole year in three areas/times
#load(file = "data/OneYearExtrapol_deeper3.RData")
#first use open water data only
#extrapData <- extrapData[which(extrapData$ice ==0),]

#there is one weird case in 2020-03-08 when larger areas had fewer anglers than smaller area (must be due to the buffer zone, or different starting points). We will remove it for now
#extrapData <- extrapData[-which(extrapData$deeper3 > extrapData$deeper1),]
# 
# ### or do it with the full 365 day dataset
#  N <- extrapData$deeper1 # sonar count full area all day
#  S <- extrapData$deeper3 # sonar count drone area 0 to 12
#  W <- extrapData$weekday # weekend

# I experimented with different initial values and they all very quickly converge, so initial values are not so important it seems 

#these are initial values to define priors
r0_current = 1 #probability of deeper given drone
a_current = 1 #deeper weekend probability multiplier

#parameters for acceptance and proposals
r0_rate = 1
a_mean = 0
a_sig = 10

### update probabilities to get final r and p 
r_current = r0_current * exp( a_current*W) # this is in a linear form (can get >1)
p_current = 1- exp( -r_current) #this is the final probability of deeper given drone on that day
#r_current
#p_current

likelihood_current = sum(dbinom(S, size=N, prob=p_current, log=TRUE))
prior_current = log_pdf_exp(r0_current,r0_rate) + log_pdf_normal(a_current, a_mean, a_sig)
posterior_current = likelihood_current + prior_current

```

# mcmc

```{r mcmc}

#_______________LOGFILE_____________
# initiate log file (provide path to your target directory)
#logfile <- here("output", "logfile1p_FullD1D3_NoIce.txt")
logfile <- here("output", "temporary.txt")
# define the column names
cat(c("it","post","likelihood","prior","r0","a","p","\n"),file=logfile,sep="\t")

# MCMC settings_____________________
n_iterations  = 200000 # original 100.000
sampling_freq = 100 # write to file every n iterations
print_freq    = 1000 # print to screen every n iterations
window_size_update_r0 = 0.04 #original 0.1
window_size_update_a = 0.7 #original 0.1

# MCMC loop
for (iteration in 0:n_iterations){
  
  #____PROPOSE NEW PARAMETERS_________
  # propose new r0
  proposal_r0 <- sliding_window(r0_current,window_size_update_r0) # use the proposal function and pre-defined window size in our settings to propose a new value for mu
  new_r0 <- proposal_r0[1]
  hastings_ratio_r0 <- proposal_r0[2]
  
  # propose new a
  proposal_a <- sliding_window(a_current,window_size_update_a) # same for sigma
  new_a <- abs(proposal_a[1]) # sigma however cannot be negative, so we use reflection at the boundary (0)
  hastings_ratio_a <- proposal_a[2]
  
  # get overall hastings ratio of new parameter proposals
  hastings_ratio <- hastings_ratio_r0 + hastings_ratio_a #+ hastings_ratio_b
  #___________________________________
  
  #__________CALC POSTERIOR___________
  # new likelihood and priors
  new_r = new_r0 * exp( new_a*W)
  new_p = 1 - exp( -new_r)
  new_lik <- sum(dbinom(S, size=N, prob=new_p, log=TRUE)) # calc likelihood under the new proposed parameter values
  new_prior <- log_pdf_exp(new_r0,r0_rate) + log_pdf_normal(new_a, a_mean, a_sig) #+ log_pdf_normal(new_b, b_mean, b_sig) 
  #new_prior <- log_pdf_exp(new_r0,r0_rate) + log_pdf_normal(new_a, a_mean, a_sig) + log_pdf_normal(new_b, b_mean, b_sig) # calculate the prior probability of the new parameter values (remember to add together the log-probs of the individual parameters)
  new_posterior <- new_lik + new_prior # apply Bayes theorem to calculate the new posterior
  #___________________________________
  
  #__________ACCEPTANCE CONDITION___________
  # calculate posterior ratio 
  posterior_ratio = new_posterior - posterior_current
  # get acceptance ratio from posterior ratio and Hastings ratio
  r = posterior_ratio + hastings_ratio
  #r
  # accept or reject
  random_number = runif(1)
  if (log(random_number) <= r){
    # if new state accepted, set current parameters to the new ones
    r0_current = new_r0
    a_current = new_a
    #b_current = new_b
    p_current = new_p
    likelihood_current = new_lik
    prior_current = new_prior
    posterior_current = new_posterior
  }
  # print to screen
  if (iteration %% print_freq == 0){
    #print(c(iteration,likelihood_current, prior_current, r0_current, a_current, b_current, mean(p_current)))
    print(c(iteration,likelihood_current, prior_current, r0_current, a_current, mean(p_current)))
  }
  # save to file
  if (iteration %% sampling_freq == 0){
    #cat(c(iteration, posterior_current, likelihood_current, prior_current, r0_current, a_current, b_current, mean(p_current),"\n"),sep="\t",file=logfile,append=T)
    cat(c(iteration, posterior_current, likelihood_current, prior_current, r0_current, a_current, mean(p_current),"\n"),sep="\t",file=logfile,append=T)
  }
}


```

# results

```{r}
#fit <- read.table(file = "output/logfile1p_mockdata.txt", header = T)

fit <- read.table(file = "output/logfile1p_deeper3_Ice.txt", header = T)
#fit <- read.table(file = "output/logfile1p_deeper3_NoIce.txt", header = T)

# fit <- read.table(file = "output/logfile1p_deeper2_Ice.txt", header = T)
# fit <- read.table(file = "output/logfile1p_deeper2_NoIce.txt", header = T)

#or with full deeper data comparing angler ratios throughout the year
#fit <- read.table(file = "output/logfile1p_FullD1D3_NoIce.txt", header = T)
#fit <- read.table(file = "output/logfile1p_FullD1D3_Ice.txt", header = T)

# plot(fit$likelihood, type = 'l')
burnin <- 100
fit <- fit[-c(1:burnin),]
plot(fit$likelihood, type = 'l')

## look at parameter quantiles 
r0q <- c(quantile(fit$r0, probs = 0.025),quantile(fit$r0, probs = 0.100),quantile(fit$r0, probs = 0.500),quantile(fit$r0, probs = 0.900),quantile(fit$r0, probs = 0.975))
aq <- c(quantile(fit$a, probs = 0.025),quantile(fit$a, probs = 0.100),quantile(fit$a, probs = 0.500),quantile(fit$a, probs = 0.900),quantile(fit$a, probs = 0.975))
pq <- c(quantile(fit$p, probs = 0.025),quantile(fit$p, probs = 0.100),quantile(fit$p, probs = 0.500),quantile(fit$p, probs = 0.900),quantile(fit$p, probs = 0.975))
knitr::kable(rbind(r0q,aq,pq), digits= 3)

  
pd_r0 <-  ggplot(data=fit, aes(x = r0)) + 
      geom_density(fill = "wheat") +
      labs(
        x    = "r0",
        y    = "Probability density"
      ) + 
     theme_bw() +
      theme(panel.border = element_blank(), 
            panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"), legend.position = "none", axis.text.y = element_blank(), axis.text.x = element_text(size = 10), axis.title.x = element_text(size = 12)) 

pd_a <- ggplot(data=fit, aes(x = a)) + 
      geom_density(fill = "wheat") +
      labs(
        x    = "a",
        y    = "Probability density"
      ) + 
     theme_bw() +
        theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"), legend.position = "none", axis.title.y = element_blank(), axis.text.y = element_blank(), axis.text.x = element_text(size = 10), axis.title.x = element_text(size = 12)) 

pd_prior <- ggplot(data=fit, aes(x = prior)) + 
      geom_density(fill = "wheat") +
      labs(
        x    = "prior",
        y    = "Probability density"
      ) + 
     theme_bw() +
        theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"), legend.position = "none", axis.title.y = element_blank(), axis.text.y = element_blank(), axis.text.x = element_text(size = 10), axis.title.x = element_text(size = 12)) 

pd_p <- ggplot(data=fit, aes(x = p)) + 
      geom_density(fill = "wheat") +
      labs(
        x    = "p",
        y    = "Probability density"
      ) + 
     theme_bw() +
        theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"), legend.position = "none", axis.title.y = element_blank(), axis.text.y = element_blank(), axis.text.x = element_text(size = 10), axis.title.x = element_text(size = 12)) +
  #geom_text(x=0.0265, y=130, label= "Open water", size = 5, fontface = 3)
  geom_text(x=0.19, y=25, label= "Ice", size = 5, fontface = 3)

plot_grid(pd_r0, pd_a, pd_p, labels = NULL, ncol = 3, nrow =1)

```


Note: with a mock data when probability on weekends is exactly the same, and the proportion is set at 0.1 we get these parameters
|    |  2.5%|   10%|   50%|   90%| 97.5%|
|:---|-----:|-----:|-----:|-----:|-----:|
|r0q | 0.087| 0.093| 0.102| 0.111| 0.116|
|aq  | 0.004| 0.014| 0.072| 0.179| 0.258|
|pq  | 0.089| 0.093| 0.100| 0.108| 0.112|



# extrapolation

### extrapolate 

```{r}

#load one year of deeper data
load(file = "data/OneYearExtrapol_deeper3.RData")

#total number of sonar counts per year 

openWeekday <- extrapData %>% group_by(ice, weekday) %>% summarise (an = sum(deeper3))

## first we use open water data only
fit <- read.table(file = "output/logfile1p_deeper3_noIce.txt", header = T)
burnin <- 100
fit <- fit[-c(1:burnin),]
plot(fit$likelihood, type = 'l')

df <- openWeekday %>% filter (ice == 0)

## get parameter quantiles 
r0q <- c(quantile(fit$r0, probs = 0.025),quantile(fit$r0, probs = 0.100),quantile(fit$r0, probs = 0.500),quantile(fit$r0, probs = 0.900),quantile(fit$r0, probs = 0.975))
aq <- c(quantile(fit$a, probs = 0.025),quantile(fit$a, probs = 0.100),quantile(fit$a, probs = 0.500),quantile(fit$a, probs = 0.900),quantile(fit$a, probs = 0.975))
pq <- c(quantile(fit$p, probs = 0.025),quantile(fit$p, probs = 0.100),quantile(fit$p, probs = 0.500),quantile(fit$p, probs = 0.900),quantile(fit$p, probs = 0.975))

### total number in open season: 50%
r_current = r0q[3] * exp( aq[3]*df$weekday) # this is in a linear form 
p_current = 1- exp( -r_current) 
angler_50q_o <- df$an*(1/p_current)

r_current = r0q[1] * exp( aq[1]*df$weekday) # this is in a linear form 
p_current = 1- exp( -r_current) 
angler_025q_o <- df$an*(1/p_current)

r_current = r0q[5] * exp( aq[5]*df$weekday) # this is in a linear form 
p_current = 1- exp( -r_current) 
angler_975q_o <- df$an*(1/p_current)

### now for ice
## first we use open water data only
fit <- read.table(file = "output/logfile1p_deeper3_Ice.txt", header = T)
burnin <- 100
fit <- fit[-c(1:burnin),]
plot(fit$likelihood, type = 'l')

df <- openWeekday %>% filter (ice == 1)

## get parameter quantiles 
r0q <- c(quantile(fit$r0, probs = 0.025),quantile(fit$r0, probs = 0.100),quantile(fit$r0, probs = 0.500),quantile(fit$r0, probs = 0.900),quantile(fit$r0, probs = 0.975))
aq <- c(quantile(fit$a, probs = 0.025),quantile(fit$a, probs = 0.100),quantile(fit$a, probs = 0.500),quantile(fit$a, probs = 0.900),quantile(fit$a, probs = 0.975))
pq <- c(quantile(fit$p, probs = 0.025),quantile(fit$p, probs = 0.100),quantile(fit$p, probs = 0.500),quantile(fit$p, probs = 0.900),quantile(fit$p, probs = 0.975))

### total number in open season: 50%
r_current = r0q[3] * exp( aq[3]*df$weekday) # this is in a linear form 
p_current = 1- exp( -r_current) 
angler_50q_i <- df$an*(1/p_current)

r_current = r0q[1] * exp( aq[1]*df$weekday) # this is in a linear form 
p_current = 1- exp( -r_current) 
angler_025q_i <- df$an*(1/p_current)

r_current = r0q[5] * exp( aq[5]*df$weekday) # this is in a linear form 
p_current = 1- exp( -r_current) 
angler_975q_i <- df$an*(1/p_current)

#total number of anglers per year in surveyed area and time
sum(angler_50q_o, angler_50q_i)
sum(angler_025q_o, angler_025q_i)
sum(angler_975q_o, angler_975q_i)

## total area 

#for open water season only
sum(angler_50q_o)
sum(angler_025q_o)
sum(angler_975q_o)

## to fully propagate error we get lower and upper intevals also from useing lower and upper intervals of angler proportion (in Table 2)
sum(angler_50q_o)/0.255
sum(angler_025q_o)/0.232
sum(angler_975q_o)/0.28

#for ice season only
sum(angler_50q_i)
sum(angler_025q_i)
sum(angler_975q_i)

sum(angler_50q_i)/0.203
sum(angler_025q_i)/0.185
sum(angler_975q_i)/0.222

sum(angler_50q_o)/0.255 + sum(angler_50q_i)/0.203
sum(angler_025q_o)/0.232 + sum(angler_025q_i)/0.185
sum(angler_975q_o)/0.28 + sum(angler_975q_i)/0.222

```

